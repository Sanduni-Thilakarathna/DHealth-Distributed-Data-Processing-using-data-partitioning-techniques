{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29070ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Experiment: Hash partitioning for GroupBy(User_ID) -> avg(Heart_Rate) ===\n",
    "# Produces: /data/exp_hash_user_avg_results.csv\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import os, time\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Config ----------\n",
    "SOURCE_CSV  = \"file:///data/cleaned_personal_health_data.csv\"   # adjust if different\n",
    "OUTPUT_CSV  = \"/data/exp_hash_user_avg_results.csv\"\n",
    "PARTITIONS_LIST = [2, 4, 8, 16, 32]   # change if you want more/fewer\n",
    "RUNS_PER_SETTING = 5                  # repeats per partition count for stability\n",
    "LIMIT_USERS = int(os.environ.get(\"LIMIT_USERS\", \"0\"))  # optional: subset for speed (0 = no limit)\n",
    "SPARK_SHUFFLE_PARTS = 4               # keep constant for fairness\n",
    "\n",
    "# ---------- Spark session (reuse existing if present) ----------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exp-Hash-UserAvg\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", str(SPARK_SHUFFLE_PARTS)) \\\n",
    "    .config(\"spark.speculation\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ---------- Load & prep ----------\n",
    "df = spark.read.csv(SOURCE_CSV, header=True, inferSchema=True)\n",
    "df = df.select(\"User_ID\", \"Heart_Rate\")  # only what we need\n",
    "\n",
    "# Optional: restrict to a subset of users (useful on slow disks / large data)\n",
    "if LIMIT_USERS > 0:\n",
    "    subset = df.select(\"User_ID\").distinct().limit(LIMIT_USERS)\n",
    "    df = df.join(subset, on=\"User_ID\", how=\"inner\")\n",
    "    print(f\"[INFO] LIMIT_USERS={LIMIT_USERS} -> rows after filter: {df.count()}\")\n",
    "\n",
    "# Warm-up read/parse\n",
    "_ = df.limit(1).count()\n",
    "\n",
    "results_rows = []\n",
    "\n",
    "for P in PARTITIONS_LIST:\n",
    "    # Hash partitioning on the key\n",
    "    base = df.repartition(P, \"User_ID\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    # Materialize cache to remove first-run noise from timings\n",
    "    _ = base.count()\n",
    "\n",
    "    # One not-timed warm-up of the exact query\n",
    "    _ = base.groupBy(\"User_ID\").agg(F.avg(\"Heart_Rate\").alias(\"Avg_Heart_Rate\")).count()\n",
    "\n",
    "    times = []\n",
    "    for r in range(1, RUNS_PER_SETTING + 1):\n",
    "        t0 = time.perf_counter()\n",
    "        # Action: triggers full shuffle+agg\n",
    "        _ = base.groupBy(\"User_ID\").agg(F.avg(\"Heart_Rate\").alias(\"Avg_Heart_Rate\")).count()\n",
    "        elapsed = round(time.perf_counter() - t0, 6)\n",
    "        times.append(elapsed)\n",
    "        print(f\"[hash] P={P:>2} run {r}/{RUNS_PER_SETTING}: {elapsed:.6f}s\")\n",
    "\n",
    "    avg_t = round(sum(times) / len(times), 6)\n",
    "    results_rows.append({\n",
    "        \"Partitioning Type\": \"hash\",\n",
    "        \"Partitions\": P,\n",
    "        \"Runs\": RUNS_PER_SETTING,\n",
    "        \"Per-Run Times (s)\": \";\".join(map(str, times)),\n",
    "        \"Avg Execution Time (s)\": avg_t,\n",
    "        \"LimitUsers\": LIMIT_USERS\n",
    "    })\n",
    "    # free cache for this setting\n",
    "    base.unpersist()\n",
    "\n",
    "# ---------- Save CSV ----------\n",
    "pdf = pd.DataFrame(results_rows)\n",
    "pdf.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n[OK] Wrote {OUTPUT_CSV}\")\n",
    "display(pdf)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
