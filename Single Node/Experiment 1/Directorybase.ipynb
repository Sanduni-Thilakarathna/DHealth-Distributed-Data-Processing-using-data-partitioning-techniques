{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbe709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Experiment: Directory partitioning for GroupBy(User_ID) -> avg(Heart_Rate) ===\n",
    "# Produces: /data/exp_directory_user_avg_results.csv\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import os, time, pandas as pd\n",
    "\n",
    "# ---------- Config ----------\n",
    "SOURCE_CSV  = \"/content/cleaned_personal_health_data.csv\"\n",
    "OUTPUT_CSV  = \"/content/Output/exp_directory_user_avg_results.csv\"\n",
    "DIR_BASE    = \"/content/Output/output_directory_partitioned_userid\"  # shared location\n",
    "FIXED_PARTITIONS = 4      # for aggregation\n",
    "RUNS = 5                  # number of timed runs\n",
    "LIMIT_USERS = int(os.environ.get(\"LIMIT_USERS\", \"0\"))\n",
    "\n",
    "# ---------- Spark ----------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exp-Directory-UserAvg\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", str(FIXED_PARTITIONS)) \\\n",
    "    .config(\"spark.speculation\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ---------- Step 1: Load base CSV ----------\n",
    "df = spark.read.csv(SOURCE_CSV, header=True, inferSchema=True)\n",
    "df = df.select(\"User_ID\", \"Heart_Rate\")\n",
    "\n",
    "# Optional subset\n",
    "if LIMIT_USERS > 0:\n",
    "    subset = df.select(\"User_ID\").distinct().limit(LIMIT_USERS)\n",
    "    df = df.join(subset, on=\"User_ID\", how=\"inner\")\n",
    "    print(f\"[INFO] LIMIT_USERS={LIMIT_USERS} -> rows after filter: {df.count()}\")\n",
    "\n",
    "# ---------- Step 2: Write directory-partitioned dataset ----------\n",
    "(\n",
    "    df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"User_ID\")\n",
    "    .parquet(DIR_BASE)\n",
    ")\n",
    "print(f\"[OK] Wrote partitioned dataset to {DIR_BASE}\")\n",
    "\n",
    "# ---------- Step 3: Read back ----------\n",
    "dfp = (\n",
    "    spark.read\n",
    "         .option(\"basePath\", DIR_BASE)\n",
    "         .parquet(f\"{DIR_BASE}/*\")\n",
    "         .select(\"User_ID\", \"Heart_Rate\")\n",
    ")\n",
    "dfp = dfp.repartition(FIXED_PARTITIONS, \"User_ID\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Warm-up\n",
    "_ = dfp.groupBy(\"User_ID\").agg(F.avg(\"Heart_Rate\").alias(\"Avg_Heart_Rate\")).count()\n",
    "\n",
    "# ---------- Step 4: Timed runs ----------\n",
    "times = []\n",
    "for r in range(1, RUNS+1):\n",
    "    t0 = time.perf_counter()\n",
    "    _ = dfp.groupBy(\"User_ID\").agg(F.avg(\"Heart_Rate\").alias(\"Avg_Heart_Rate\")).count()\n",
    "    elapsed = round(time.perf_counter() - t0, 6)\n",
    "    times.append(elapsed)\n",
    "    print(f\"[directory] run {r}/{RUNS}: {elapsed:.6f}s\")\n",
    "\n",
    "avg_t = round(sum(times)/len(times), 6)\n",
    "\n",
    "# ---------- Step 5: Save ----------\n",
    "results = pd.DataFrame([{\n",
    "    \"Partitioning Type\": \"directory\",\n",
    "    \"Partitions\": FIXED_PARTITIONS,\n",
    "    \"Runs\": RUNS,\n",
    "    \"Per-Run Times (s)\": \";\".join(map(str, times)),\n",
    "    \"Avg Execution Time (s)\": avg_t,\n",
    "    \"LimitUsers\": LIMIT_USERS\n",
    "}])\n",
    "results.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n[OK] Wrote {OUTPUT_CSV}\")\n",
    "display(results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
